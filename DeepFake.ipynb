{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"1fbw3E8PutNPiQZlgVj5iJfI85yNXYr-N","authorship_tag":"ABX9TyOs/a7pXisDUThzdPWrjRLF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d6a6ebab4866475f93afae8184861e97":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_351ca2802e0842c3bad93f63df8144fa","IPY_MODEL_ba4cb1a29f0c40b4b044a0b1d5dded08","IPY_MODEL_2d4f7036cebb46ecbc6a2982808aea3c"],"layout":"IPY_MODEL_46077d6ad621453a828cbb6598cce22d"}},"351ca2802e0842c3bad93f63df8144fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ef1454d2a949cea0ccb014865a5c16","placeholder":"​","style":"IPY_MODEL_d6903a26c21f4552932942196c758990","value":"100%"}},"ba4cb1a29f0c40b4b044a0b1d5dded08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f16200c995c41308752b88fcf455535","max":590,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63870aa3aa694cdda565db28b2131c31","value":590}},"2d4f7036cebb46ecbc6a2982808aea3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51164d8b909543c7b26f338aa55d71c6","placeholder":"​","style":"IPY_MODEL_c6fece1cfe234d58a7fd01d13c43a2cc","value":" 590/590 [04:37&lt;00:00, 11.22s/it]"}},"46077d6ad621453a828cbb6598cce22d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30ef1454d2a949cea0ccb014865a5c16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6903a26c21f4552932942196c758990":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f16200c995c41308752b88fcf455535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63870aa3aa694cdda565db28b2131c31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51164d8b909543c7b26f338aa55d71c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6fece1cfe234d58a7fd01d13c43a2cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cf62aed595b44db9845c4875b0a9a08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4577daa6c054d049f5f191c241d4a93","IPY_MODEL_7b8f2ee96bd04070b95705a0292d0a6c","IPY_MODEL_30a157c2df364dbd99f046ab02934717"],"layout":"IPY_MODEL_a5a56921e8dd47fb8564c640446add30"}},"e4577daa6c054d049f5f191c241d4a93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7be787a78ec4ae5b87d67ad1cf16d1e","placeholder":"​","style":"IPY_MODEL_7f000c4d0d3f475aa56e89fce4ab3c56","value":" 16%"}},"7b8f2ee96bd04070b95705a0292d0a6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf6ce3a47b74f3d86a4f2709294bef2","max":5639,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9e09433b42b4547b914ac220ea77d9f","value":917}},"30a157c2df364dbd99f046ab02934717":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5cc0ae853304c90ac8b670625f42e81","placeholder":"​","style":"IPY_MODEL_18f58b28dc0b42069c17c0191cb5a182","value":" 917/5639 [3:11:14&lt;18:22:22, 14.01s/it]"}},"a5a56921e8dd47fb8564c640446add30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7be787a78ec4ae5b87d67ad1cf16d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f000c4d0d3f475aa56e89fce4ab3c56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cf6ce3a47b74f3d86a4f2709294bef2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e09433b42b4547b914ac220ea77d9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5cc0ae853304c90ac8b670625f42e81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18f58b28dc0b42069c17c0191cb5a182":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McIGVUO1bcF3","executionInfo":{"status":"ok","timestamp":1724615957378,"user_tz":-330,"elapsed":26697,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"ac7ba8f5-6844-45f6-e734-95152f27c3f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face-recognition==1.3.0\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting face-recognition-models>=0.3.0 (from face-recognition==1.3.0)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (8.1.7)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (19.24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (9.4.0)\n","Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=db1f3110631bfd80763d08fe39df4daf21a6aea9c5e92ab65b2e00cf68a0821b\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face-recognition\n","Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"]}],"source":["pip install face-recognition==1.3.0"]},{"cell_type":"code","source":["pip install tensorflow==2.11.0 opencv-python-headless==4.5.5.64 face-recognition==1.3.0 numpy==1.21.6 tqdm==4.64.1 glob2==0.7 matplotlib==3.5.1 pillow==9.0.1 scikit-learn==1.0.2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dcqkhlHCEKOo","executionInfo":{"status":"ok","timestamp":1724615471447,"user_tz":-330,"elapsed":160558,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"4bb7dcef-c3d2-4661-8263-fcd4213146e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.11.0\n","  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n","Collecting opencv-python-headless==4.5.5.64\n","  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting face-recognition==1.3.0\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting numpy==1.21.6\n","  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Collecting tqdm==4.64.1\n","  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: glob2==0.7 in /usr/local/lib/python3.10/dist-packages (0.7)\n","Collecting matplotlib==3.5.1\n","  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting pillow==9.0.1\n","  Downloading Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Collecting scikit-learn==1.0.2\n","  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n","Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n","  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n","Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n","  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n","Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n","Collecting face-recognition-models>=0.3.0 (from face-recognition==1.3.0)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (8.1.7)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face-recognition==1.3.0) (19.24.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1) (2.8.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n","INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n","Collecting scipy>=1.1.0 (from scikit-learn==1.0.2)\n","  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n","Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=1cdb2eacd04f9f92c29f88999c63f1185ad04d448ffc3577dcb1f9689c57076b\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: tensorboard-plugin-wit, face-recognition-models, tqdm, tensorflow-estimator, tensorboard-data-server, protobuf, pillow, numpy, keras, gast, scipy, opencv-python-headless, matplotlib, face-recognition, scikit-learn, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.5\n","    Uninstalling tqdm-4.66.5:\n","      Successfully uninstalled tqdm-4.66.5\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.10.0.84\n","    Uninstalling opencv-python-headless-4.10.0.84:\n","      Successfully uninstalled opencv-python-headless-4.10.0.84\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.3.2\n","    Uninstalling scikit-learn-1.3.2:\n","      Successfully uninstalled scikit-learn-1.3.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n","albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n","albucore 0.0.13 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n","albumentations 1.4.14 requires numpy>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n","albumentations 1.4.14 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n","arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n","astropy 6.1.2 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n","bigframes 1.15.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.1 which is incompatible.\n","bigframes 1.15.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.0.2 which is incompatible.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.21.6 which is incompatible.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n","cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","flax 0.8.4 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","geopandas 0.14.4 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","google-cloud-aiplatform 1.63.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-pubsub 2.23.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","jax 0.4.26 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","jaxlib 0.4.26+cuda12.cudnn89 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 1.21.6 which is incompatible.\n","numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n","pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 1.21.6 which is incompatible.\n","pandas-gbq 0.23.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.21.6 which is incompatible.\n","plotnine 0.12.4 requires matplotlib>=3.6.0, but you have matplotlib 3.5.1 which is incompatible.\n","plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n","rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n","scikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n","scikit-image 0.23.2 requires pillow>=9.1, but you have pillow 9.0.1 which is incompatible.\n","statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.21.6 which is incompatible.\n","tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n","tensorstore 0.1.64 requires numpy>=1.22.0, but you have numpy 1.21.6 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\n","xarray 2024.6.0 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n","xarray-einstats 0.7.0 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed face-recognition-1.3.0 face-recognition-models-0.3.0 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 matplotlib-3.5.1 numpy-1.21.6 opencv-python-headless-4.5.5.64 pillow-9.0.1 protobuf-3.19.6 scikit-learn-1.0.2 scipy-1.11.4 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tqdm-4.64.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","google","matplotlib","mpl_toolkits","numpy"]},"id":"e4b9f5752b64477ca392341e8d7987f3"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLub_odZGMoy","executionInfo":{"status":"ok","timestamp":1724615878958,"user_tz":-330,"elapsed":17910,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"d8c9c1af-54f7-4ec0-87c1-f02a95fb37e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Kk-S1c9RGLcm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#directory to save real processed videos\n","import os\n","\n","mkdir_path = '/content/drive/MyDrive/DeepFake/Celeb_real_face_only'\n","if not os.path.exists(mkdir_path):\n","  os.mkdir(mkdir_path)\n","\n","mkdir_path = '/content/drive/MyDrive/DeepFake/Celeb_fake_face_only'\n","if not os.path.exists(mkdir_path):\n","  os.mkdir(mkdir_path)"],"metadata":{"id":"9iA0lXSPYzA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install face_recognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HkfNZ3WbIWf","executionInfo":{"status":"ok","timestamp":1724663639108,"user_tz":-330,"elapsed":31176,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"2c4cee78-9bf2-4351-ddff-4385b52606e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting face-recognition-models>=0.3.0 (from face_recognition)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n","Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=b5a361782abd16e607ad2ccac9227c3520fe69e74cee8cbfcce485f3854e432c\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}]},{"cell_type":"code","source":["import cv2\n","import os\n","import face_recognition\n","from tqdm.autonotebook import tqdm\n","import glob\n","\n","# Function to create videos containing only face frames\n","def create_face_videos(path_list, output_dir):\n","    # Find and count how many videos are already present in the output directory\n","    old_videos = glob.glob(output_dir + '*.mp4')\n","    print(\"No of videos already present:\", len(old_videos))\n","\n","    # Iterate through each video path in the provided path list\n","    for path in tqdm(path_list):\n","        # Construct the output path for the video\n","        output_path = os.path.join(output_dir, path.split('/')[-1])\n","\n","        # Check if the output video already exists\n","        if glob.glob(output_path):\n","            #print(\"File Already exists:\", output_path)\n","            continue  # Skip to the next video if it already exists\n","\n","        frames = []  # List to store frames temporarily\n","        flag = 0\n","        face_all = []\n","        frames1 = []\n","\n","        # Create a VideoWriter object to save the output video with specified codec and resolution\n","        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 30, (128, 128))\n","\n","        # Iterate over each frame extracted from the video\n","        for idx, frame in enumerate(frame_extract(path)):\n","            # Process only the first 200 frames of the video\n","            if idx <= 200:\n","                frames.append(frame)  # Add the frame to the list\n","\n","                # Process in batches of 4 frames to detect faces\n","                if len(frames) == 4:\n","                    # Detect faces in the current batch of frames\n","                    faces = face_recognition.batch_face_locations(frames)\n","\n","                    # Iterate over detected faces in each frame\n","                    for i, face in enumerate(faces):\n","                        if face:  # If a face is detected\n","                            top, right, bottom, left = face[0]  # Get the coordinates of the face\n","\n","                        try:\n","                            # Crop, resize the face, and write it to the output video\n","                            out.write(cv2.resize(frames[i][top:bottom, left:right, :], (128, 128)))\n","                        except:\n","                            pass  # Skip if an error occurs during face extraction\n","\n","                    # Clear the frames list for the next batch\n","                    frames = []\n","\n","        # Clean up by deleting the face coordinates if they exist\n","        try:\n","            del top, right, bottom, left\n","        except:\n","            pass\n","\n","        # Release the VideoWriter object after processing the video\n","        out.release()\n","\n","\n","# Specify the list of video paths and the output directory\n","video_paths = glob.glob('/content/drive/MyDrive/DeepFake/Celeb-real/*.mp4')\n","output_directory = '/content/drive/MyDrive/DeepFake/Celeb_real_face_only'\n","\n","print(len(video_paths))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kM97NrtAXP8R","executionInfo":{"status":"ok","timestamp":1724664588988,"user_tz":-330,"elapsed":452,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"9d58ed64-d902-4672-9eef-2e45ff919161"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["590\n"]}]},{"cell_type":"code","source":["create_face_videos(video_paths, output_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d6a6ebab4866475f93afae8184861e97","351ca2802e0842c3bad93f63df8144fa","ba4cb1a29f0c40b4b044a0b1d5dded08","2d4f7036cebb46ecbc6a2982808aea3c","46077d6ad621453a828cbb6598cce22d","30ef1454d2a949cea0ccb014865a5c16","d6903a26c21f4552932942196c758990","2f16200c995c41308752b88fcf455535","63870aa3aa694cdda565db28b2131c31","51164d8b909543c7b26f338aa55d71c6","c6fece1cfe234d58a7fd01d13c43a2cc"]},"id":"m_QyInMiigot","executionInfo":{"status":"ok","timestamp":1724664869619,"user_tz":-330,"elapsed":277529,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"be9a2bdd-8272-46b4-eeef-bd5049a46ca9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No of videos already present: 0\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/590 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a6ebab4866475f93afae8184861e97"}},"metadata":{}}]},{"cell_type":"code","source":["# Function to create videos containing only face frames\n","def create_face_videos(path_list, output_dir):\n","    # Find and count how many videos are already present in the output directory\n","    old_videos = glob.glob(output_dir + '*.mp4')\n","    print(\"No of videos already present:\", len(old_videos))\n","    cnt=0\n","    # Iterate through each video path in the provided path list\n","    for path in tqdm(path_list):\n","        cnt+=1\n","        if cnt==1001:\n","          break\n","        # Construct the output path for the video\n","        output_path = os.path.join(output_dir, path.split('/')[-1])\n","\n","        # Check if the output video already exists\n","        if glob.glob(output_path):\n","            #print(\"File Already exists:\", output_path)\n","            continue  # Skip to the next video if it already exists\n","\n","        frames = []  # List to store frames temporarily\n","        flag = 0\n","        face_all = []\n","        frames1 = []\n","\n","        # Create a VideoWriter object to save the output video with specified codec and resolution\n","        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 30, (128, 128))\n","\n","        # Iterate over each frame extracted from the video\n","        for idx, frame in enumerate(frame_extract(path)):\n","            # Process only the first 200 frames of the video\n","            if idx <= 200:\n","                frames.append(frame)  # Add the frame to the list\n","\n","                # Process in batches of 4 frames to detect faces\n","                if len(frames) == 4:\n","                    # Detect faces in the current batch of frames\n","                    faces = face_recognition.batch_face_locations(frames)\n","\n","                    # Iterate over detected faces in each frame\n","                    for i, face in enumerate(faces):\n","                        if face:  # If a face is detected\n","                            top, right, bottom, left = face[0]  # Get the coordinates of the face\n","\n","                        try:\n","                            # Crop, resize the face, and write it to the output video\n","                            out.write(cv2.resize(frames[i][top:bottom, left:right, :], (128, 128)))\n","                        except:\n","                            pass  # Skip if an error occurs during face extraction\n","\n","                    # Clear the frames list for the next batch\n","                    frames = []\n","\n","        # Clean up by deleting the face coordinates if they exist\n","        try:\n","            del top, right, bottom, left\n","        except:\n","            pass\n","\n","        # Release the VideoWriter object after processing the video\n","        out.release()\n","\n","\n","\n","\n","video_paths = glob.glob('/content/drive/MyDrive/DeepFake/Celeb-synthesis/*.mp4')\n","output_directory = '/content/drive/MyDrive/DeepFake/Celeb_fake_face_only'\n","\n","print(len(video_paths))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0PmV32lCrvA","executionInfo":{"status":"ok","timestamp":1724665257682,"user_tz":-330,"elapsed":444,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}},"outputId":"8801aaca-cf5a-4833-9f41-708d116f210a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5639\n"]}]},{"cell_type":"code","source":["create_face_videos(video_paths, output_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353,"referenced_widgets":["2cf62aed595b44db9845c4875b0a9a08","e4577daa6c054d049f5f191c241d4a93","7b8f2ee96bd04070b95705a0292d0a6c","30a157c2df364dbd99f046ab02934717","a5a56921e8dd47fb8564c640446add30","c7be787a78ec4ae5b87d67ad1cf16d1e","7f000c4d0d3f475aa56e89fce4ab3c56","3cf6ce3a47b74f3d86a4f2709294bef2","f9e09433b42b4547b914ac220ea77d9f","d5cc0ae853304c90ac8b670625f42e81","18f58b28dc0b42069c17c0191cb5a182"]},"id":"enoGHiWRDxKv","outputId":"ae8999f8-4341-4eb6-d15b-db45d14b1086","executionInfo":{"status":"error","timestamp":1724676739153,"user_tz":-330,"elapsed":888290,"user":{"displayName":"Hritik Raushan","userId":"05413437437446408909"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["No of videos already present: 0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cf62aed595b44db9845c4875b0a9a08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5639 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-8962e0892851>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_face_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-0fad399151ce>\u001b[0m in \u001b[0;36mcreate_face_videos\u001b[0;34m(path_list, output_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;31m# Detect faces in the current batch of frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;31m# Iterate over detected faces in each frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mbatch_face_locations\u001b[0;34m(images, number_of_times_to_upsample, batch_size)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mraw_detections_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_locations_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_cnn_detections_to_css\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_detections_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations_batched\u001b[0;34m(images, number_of_times_to_upsample, batch_size)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdlib\u001b[0m \u001b[0;34m'rect'\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mof\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mface\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}